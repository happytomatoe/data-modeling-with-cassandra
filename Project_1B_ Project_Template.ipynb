{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I. ETL Pipeline for Pre-Processing the Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Python packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T16:27:20.595235Z",
     "start_time": "2021-10-09T16:27:20.584588Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Python packages\n",
    "import pandas as pd\n",
    "import cassandra\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating list of filepaths to process original event csv data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T16:27:20.610012Z",
     "start_time": "2021-10-09T16:27:20.602571Z"
    }
   },
   "outputs": [],
   "source": [
    "filepath = os.getcwd() + '/event_data'\n",
    "\n",
    "for root, dirs, files in os.walk(filepath):\n",
    "    file_path_list = glob.glob(os.path.join(root,'*'))\n",
    "    #print(file_path_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing the files to create the data file csv that will be used for Apache Casssandra tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T16:27:20.692497Z",
     "start_time": "2021-10-09T16:27:20.614452Z"
    }
   },
   "outputs": [],
   "source": [
    "class ColumnNames:\n",
    "    artist = \"artist\"\n",
    "    first_name = \"firstName\"\n",
    "    last_name = \"lastName\"\n",
    "    gender = \"gender\"\n",
    "    item_in_session = \"itemInSession\"\n",
    "    length = \"length\"\n",
    "    level = \"level\"\n",
    "    location = \"location\"\n",
    "    session_id = \"sessionId\"\n",
    "    user_id = \"userId\"\n",
    "    song = \"song\"\n",
    "\n",
    "\n",
    "full_data_rows_list = []\n",
    "file = 'event_datafile_new.csv'\n",
    "\n",
    "\n",
    "for f in file_path_list:\n",
    "\n",
    "    with open(f, 'r', encoding='utf8', newline='') as csvfile:\n",
    "        csvreader = csv.reader(csvfile)\n",
    "        next(csvreader)\n",
    "\n",
    "        for line in csvreader:\n",
    "            # print(line)\n",
    "            full_data_rows_list.append(line)\n",
    "\n",
    "# creating a smaller event data csv file called event_datafile_full csv\n",
    "# that will be used to insert data into the  Apache Cassandra tables\n",
    "csv.register_dialect('myDialect', quoting=csv.QUOTE_ALL, skipinitialspace=True)\n",
    "\n",
    "with open(file, 'w', encoding='utf8', newline='') as f:\n",
    "    writer = csv.writer(f, dialect='myDialect')\n",
    "    writer.writerow([ColumnNames.artist, ColumnNames.first_name, ColumnNames.gender,ColumnNames.item_in_session, ColumnNames.last_name, ColumnNames.length,ColumnNames.level, ColumnNames.location, ColumnNames.session_id, ColumnNames.song, ColumnNames.user_id])\n",
    "    for row in full_data_rows_list:\n",
    "        if (row[0] == ''):\n",
    "            continue\n",
    "        writer.writerow((row[0], row[2], row[3], row[4], row[5],\n",
    "                        row[6], row[7], row[8], row[12], row[13], row[16]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing the files to create the data file csv that will be used for Apache Casssandra tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T16:27:20.700217Z",
     "start_time": "2021-10-09T16:27:20.694357Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6821\n"
     ]
    }
   ],
   "source": [
    "# checking the number of rows in the csv file\n",
    "with open(file, 'r', encoding = 'utf8') as f:\n",
    "    print(sum(1 for line in f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T16:27:20.738508Z",
     "start_time": "2021-10-09T16:27:20.702290Z"
    }
   },
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "cluster = Cluster(['db'])\n",
    "\n",
    "session = cluster.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Keyspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T16:27:20.746755Z",
     "start_time": "2021-10-09T16:27:20.740329Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x7f75a2d405b0>"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.execute(\"\"\"\n",
    "CREATE KEYSPACE IF NOT EXISTS sparkify\n",
    "  WITH REPLICATION = { \n",
    "   'class' : 'SimpleStrategy', \n",
    "   'replication_factor' : 1 \n",
    "  };\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Keyspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T16:27:20.756456Z",
     "start_time": "2021-10-09T16:27:20.750623Z"
    }
   },
   "outputs": [],
   "source": [
    "session.set_keyspace(\"sparkify\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II. Create queries to ask the following three questions of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Question 1. Give me the artist, song title and song's length in the music app history that was heard during  sessionId = 338, and itemInSession  = 4\n",
    "\n",
    "\n",
    "The Question 1 expects artist, song title and length of a song based on the sessionId and itemInSession attributes.  \n",
    "\n",
    "As we are working with a NoSQL database, we need to think about the query first which will be used to fetch the data based on which we will create the Table required.\n",
    "\n",
    "\n",
    "1. The expected output is : \"artist, song title and length of a song\"\n",
    "2. Based on : \"sessionId and itemInSession\"\n",
    "\n",
    "From the above two points we know the query to get the data will be a SELECT statement like :\n",
    "\n",
    "`SELECT artist, title of a song, length of a song FROM TABLE_NAME WHERE sessionId = value And itemInSession = value`\n",
    "\n",
    "As we know the SELECT query, we can move to CREATE table query. We will add `NOT EXIST` to the `CREATE` statement to check if the table exists and only create the table if it does not exist. Now we need to select the columns that are going to be in the table and the PRIMARY KEY.\n",
    "\n",
    "**Column Names:**\n",
    "\n",
    "As the query expects artist, title of a song and length of a song on query upon sessionId and itemInSession. Hence the table should have artist, song_title, song_length, session_id and item_in_session columns.\n",
    "\n",
    "**Primary Key:**\n",
    "\n",
    "The Primary key for the table should uniquely identify each row in the table. Primary key consists of partition key(which will be used by db to know to which node to route specific query request) and clustering columns(which are used to order rows inside dataset). \n",
    "\n",
    "**Partition key**:\n",
    "\n",
    "As we want to decrease the change of hot spot servers partition key should be as random as possible. In our case combination of session_id and item_in_session is the best choice in this situation as picking one the columns won't work with Apache Cassandra out the box. The only possible option in this case is to use `ALLOW FILTERING` option which will lead to performance degradation comparing to previous choice.\n",
    "\n",
    "**Clustering columns:**\n",
    "\n",
    "We should also add userId to guarantee Primary key uniqueness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the table and populate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T16:27:24.197912Z",
     "start_time": "2021-10-09T16:27:20.758030Z"
    }
   },
   "outputs": [],
   "source": [
    "session.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS song_session(\n",
    "   session_id bigint,\n",
    "   item_in_session int,\n",
    "   user_id bigint,\n",
    "   artist text, \n",
    "   song_title text, \n",
    "   song_length float,\n",
    "   PRIMARY KEY ((session_id, item_in_session), user_id)\n",
    "   )\n",
    "\"\"\")\n",
    "query = session.prepare(\"\"\"\n",
    "INSERT INTO song_session(session_id, item_in_session, user_id, artist, song_title, song_length) VALUES (?,?,?,?,?,?)\n",
    "\"\"\")\n",
    "\n",
    "with open(file, encoding='utf8') as f:\n",
    "    csvreader = csv.DictReader(f)\n",
    "    for line in csvreader:\n",
    "        session.execute(query, (\n",
    "            int(line[ColumnNames.session_id]),\n",
    "            int(line[ColumnNames.item_in_session]),\n",
    "            int(line[ColumnNames.user_id]),\n",
    "            line[ColumnNames.artist],\n",
    "            line[ColumnNames.song],\n",
    "            float(line[ColumnNames.length]),\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get an answer to the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T16:27:24.203224Z",
     "start_time": "2021-10-09T16:27:24.199077Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(artist='Faithless', song_title='Music Matters (Mark Knight Dub)', song_length=495.30731201171875)\n"
     ]
    }
   ],
   "source": [
    "r = session.execute(\"\"\"\n",
    "SELECT artist, song_title, song_length FROM song_session\n",
    "WHERE session_id = 338 And item_in_session = 4\n",
    "\"\"\")\n",
    "for x in r:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2. Give me only the following: name of artist, song (sorted by itemInSession) and user name (first and last name) for userid = 10, sessionid = 182\n",
    "\n",
    "The Question 2 expects name of artist, song title song (sorted by itemInSession) and user name (first and last name) based on userid and sessionid.  \n",
    "\n",
    "As we are working with a NoSQL database, we need to think about the query first which will be used to fetch the data based on which we will create the Table required.\n",
    "\n",
    "1. The expected output is : \"Artist name, song title\"\n",
    "2. Based on : \"userid and sessionid\"\n",
    "3. Ordered by itemInSession\n",
    "\n",
    "From the first two points we know the query to get the data will be a SELECT statement like:\n",
    "\n",
    "` SELECT artist, title of asong, user name FROM TABLE_NAME WHERE user_id = value And session_id = value`\n",
    "\n",
    "As we know the SELECT query, we can move to CREATE table query. We will add NOT EXIST to the CREATE statement to check if the table exists and only create the table if it does not exist. Now we need to select the columns that are going to be in the table and the PRIMARY KEY.\n",
    "\n",
    "**Column Names:**\n",
    "\n",
    "We need artist, title of a song, and user name on query upon userid and session id. And also we need itemInSession for ordering. Hence the table should have artist, username, user_id, session_id and item_in_session columns.\n",
    "\n",
    "**Primary Key:**\n",
    "\n",
    "The Primary key for the table should uniquely identify each row in the table. Primary key consists of partition key(which will be used by db to know to which node to route specific query request) and clustering columns(which are used to order rows inside dataset). \n",
    "\n",
    "**Partition key:**\n",
    "\n",
    "As we want to decrease the change of hot spot servers partition key should be as random as possible. In our case combination of session_id and user_id should give us unique values.\n",
    "\n",
    "**Clustering columns:**\n",
    "\n",
    "As for the clustering columns we will choose item_in_session as the query results should be sorted by it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-09T15:45:49.177237Z",
     "start_time": "2021-10-09T15:45:49.167581Z"
    }
   },
   "source": [
    "#### Create the table and populate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-10-09T16:27:20.623Z"
    }
   },
   "outputs": [],
   "source": [
    "session.execute(\"\"\"\n",
    "CREATE TABLE artist_listening_history(\n",
    "   session_id bigint,\n",
    "   user_id bigint,\n",
    "   item_in_session int,\n",
    "   artist text, \n",
    "   song_title text, \n",
    "   username text,\n",
    "   PRIMARY KEY ((session_id, user_id), item_in_session)\n",
    "   );\n",
    "\"\"\")\n",
    "query = session.prepare( \"\"\"\n",
    "INSERT INTO artist_listening_history(session_id, user_id, item_in_session, artist, song_title, username)\n",
    "VALUES (?,?,?,?,?,?)\n",
    "\"\"\")\n",
    "\n",
    "with open(file, encoding = 'utf8') as f:\n",
    "    csvreader = csv.DictReader(f)\n",
    "    for line in csvreader:\n",
    "        values=(int(line[ColumnNames.session_id]),\n",
    "                int(line[ColumnNames.user_id]),\n",
    "                int(line[ColumnNames.item_in_session]),\n",
    "                line[ColumnNames.artist], \n",
    "                line[ColumnNames.song],\n",
    "                line[ColumnNames.first_name]+\" \"+line[ColumnNames.last_name],\n",
    "                )\n",
    "#         print(values)\n",
    "        session.execute(query, values ) \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get an answer to the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-10-09T16:27:20.626Z"
    }
   },
   "outputs": [],
   "source": [
    "r = session.execute(\"\"\"\n",
    " SELECT artist, song_title, username FROM artist_listening_history\n",
    " WHERE user_id = 10 And session_id = 182\n",
    "\"\"\")\n",
    "\n",
    "for x in r:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3. Give me every user name (first and last) in my music app history who listened to the song 'All Hands Against His Own'\n",
    "\n",
    "\n",
    "The Question 3 expects user name based on attributes song title.  \n",
    "\n",
    "As we are working with a NoSQL database, we need to think about the query first which will be used to fetch the data based on which we will create the Table required.\n",
    "\n",
    "\n",
    "1. The expected output is : \"user name\"\n",
    "2. Based on : \"song title\"\n",
    "\n",
    "From the above two points we know the query to get the data will be a SELECT statement like :\n",
    "\n",
    "`\n",
    "SELECT username FROM TABLE_NAME WHERE song=value\n",
    "`\n",
    "\n",
    "As we know the SELECT query, we can move to CREATE table query. We will add `NOT EXIST` to the `CREATE` statement to check if the table exists and only create the table if it does not exist. Now we need to select the columns that are going to be in the table and the PRIMARY KEY.\n",
    "\n",
    "**Column Names:**\n",
    "\n",
    "As we need user name and song, hence the table should have username and song_title columns.\n",
    "\n",
    "**Primary Key:**\n",
    "\n",
    "The Primary key for the table should uniquely identify each row in the table.\n",
    "Primary key consists of partition key (which will be used by db to know to which node to route specific query request) and clustering columns (which are used to order rows inside dataset).\n",
    "\n",
    "**Partition key:**\n",
    "\n",
    "As we want to decrease the chance of hot spot servers partition key should be as random as possible. In this case the `WHERE` statement contains only  1 column - song title so will pick it as a partition column.\n",
    "\n",
    "**Clustering columns:**\n",
    "\n",
    "We will also pick username as a clustering column to guarantee Primary key uniqueness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the table and populate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-10-09T16:27:20.628Z"
    }
   },
   "outputs": [],
   "source": [
    "session.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS user_listening_history(\n",
    "   username text,\n",
    "   song text,\n",
    "   PRIMARY KEY (song, username)\n",
    "   );\n",
    "\"\"\")\n",
    "query = session.prepare( \"\"\"\n",
    "INSERT INTO user_listening_history(song,username) VALUES (?,?)\n",
    "\"\"\")\n",
    "\n",
    "with open(file, encoding = 'utf8') as f:\n",
    "    csvreader = csv.DictReader(f)\n",
    "    for line in csvreader:\n",
    "        values=(line[ColumnNames.song],\n",
    "               line[ColumnNames.first_name]+\" \"+line[ColumnNames.last_name])\n",
    "#         print(values)\n",
    "        session.execute(query, values ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get an answer to the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-10-09T16:27:20.631Z"
    }
   },
   "outputs": [],
   "source": [
    "r = session.execute(\"\"\"\n",
    "SELECT username FROM user_listening_history\n",
    "WHERE song='All Hands Against His Own'\n",
    "\"\"\")\n",
    "result = [x.username for x in r]\n",
    "print(\"Users that listened to the song 'All Hands Against His Own' - \", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop the tables before closing out the sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-10-09T16:27:20.633Z"
    }
   },
   "outputs": [],
   "source": [
    "session.execute(\"\"\"\n",
    "DROP TABLE IF EXISTS sparkify.song_session\n",
    "\"\"\")\n",
    "session.execute(\"\"\"\n",
    "DROP TABLE IF EXISTS sparkify.artist_listening_history\n",
    "\"\"\")\n",
    "session.execute(\"\"\"\n",
    "DROP TABLE IF EXISTS sparkify.user_listening_history\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close the session and cluster connection¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-10-09T16:27:20.635Z"
    }
   },
   "outputs": [],
   "source": [
    "session.shutdown()\n",
    "cluster.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}